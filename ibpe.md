---
layout: page
title: Capstone Project
permalink: /ibpe/
---

<h2>Demo Information</h2>

* STEP 1: Download your demo submission from Amathuba.
* STEP 2: Download the txt files for training and tokenizing from [here](https://drive.google.com/file/d/1xJaxLAX08LfskKvcKVW78fzRRZWddc2c/view).
* STEP 3: Start up your IBPE website/program (using the Amathuba files) on your laptop.
* STEP 4: The client will operate the IBPE tokenizer, on your machine, testing the requirements one at a time.

Please test STEPS 1--3 at home, so we don't run into any delays during the demo. If the file size constraints of Amathuba prevented you from submitting your entire program (e.g. large libraries and dependencies), just explain that your demo uses additional code bases (not code you wrote yourself) besides your Amathuba files.

	
<h2>Interactive BPE tokenizer (IBPE)</h2>



The aim of this project is to develop a web-based interface for a custom text tokenizer, similar to the [ChatGPT interactive tokenizer](https://platform.openai.com/tokenizer). Tokenization is a crucial preprocessing step in Natural Language Processing (NLP). The BPE tokenizer is a subword tokenisation algorithm that splits words into smaller subword units based on their frequency in a given text corpus. For instance, the word "unhappiness" might be tokenized into "un", "happi", and "ness". This approach is useful for handling rare words and out-of-vocabulary words by breaking them down into more common subwords. In this project, students will develop a web interface that allows users to upload a text file to create a custom BPE tokenizer. The subword frequencies for BPE will be stored in a database. Users will be able to input text to see the tokenization output, similar to the ChatGPT tokenizer. This project will provide a platform for students to experiment with the impact of tokenization on various text corpora, offering hands-on experience with a fundamental NLP technique.


<h2>Resources</h2>


* [Week 1 slides: Introducing the project](https://drive.google.com/file/d/171zX20xmgomXx_q7kFEXPPeFczeJ6Fdp/view?usp=sharing)
* [BPE algorithm explained](https://huggingface.co/learn/nlp-course/en/chapter6/5)
* [OpenAI tokenizer demo](https://platform.openai.com/tokenizer)
* [Week 2 info: BPE efficiency](https://drive.google.com/file/d/1WenFZGIfSx0XdZ-CVdpRCutpuXUEPMXz/view?usp=sharing)
* [Paper: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://aclanthology.org/2023.emnlp-main.614.pdf)

<h2>Datasets for testing BPE</h2>

* [WURA isiXhosa](https://drive.google.com/file/d/1OMiiVbsMs3RE3mbk_-Oc0C3XcmMQ-X9J/view?usp=sharing)
* [WURA full](https://huggingface.co/datasets/castorini/wura)
* [Europarl](https://www.statmt.org/europarl/)

<h2>Contact</h2>

* Supervisor/Client: Francois Meyer \
  Email: francois.meyer@uct.ac.za \
  Office: Room 316, Computer Science Building
	

* Tutor: Thomas van Coller \
  Email: VCLTHO001@myuct.ac.za


